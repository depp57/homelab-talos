# Copied from https://www.talos.dev/v1.11/kubernetes-guides/network/deploying-cilium/#method-5-using-a-job
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cilium-install
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: cilium-install
    namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-install
  namespace: kube-system
---
apiVersion: batch/v1
kind: Job
metadata:
  name: cilium-install
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        app: cilium-install
    spec:
      restartPolicy: OnFailure
      tolerations:
        - operator: Exists
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - effect: PreferNoSchedule
          operator: Exists
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoExecute
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: PreferNoSchedule
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
      serviceAccountName: cilium-install
      hostNetwork: true
      containers:
        - name: cilium-install
          image: quay.io/cilium/cilium-cli:latest
          env:
            - name: KUBERNETES_SERVICE_HOST
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: KUBERNETES_SERVICE_PORT
              value: "6443"
          command:
            - cilium
            - install
            - --set
            - ipam.mode=kubernetes
            - --set
            - kubeProxyReplacement=true
            - --set
            - securityContext.capabilities.ciliumAgent={CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}
            - --set
            - securityContext.capabilities.cleanCiliumState={NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}
            - --set
            - cgroup.autoMount.enabled=false
            - --set
            - cgroup.hostRoot=/sys/fs/cgroup
            - --set
            - k8sServiceHost=localhost
            - --set
            - k8sServicePort=7445
---
# In addition to the manifests copied from Talos doc, I automate the cleanup of the resources with the following job
apiVersion: batch/v1
kind: Job
metadata:
  name: cilium-cleanup
  namespace: kube-system
spec:
#  ttlSecondsAfterFinished: 0
  template:
    metadata:
      labels:
        app: cilium-cleanup
    spec:
      restartPolicy: OnFailure
      tolerations:
        - operator: Exists
      containers:
        - name: cilium-cleanup
          image: bitnami/kubectl:latest
          command:
            - sh
            - -c
            - |
              set -e

              echo "Waiting for cilium-install job to complete..."
              kubectl wait --for=condition=complete job/cilium-install -n kube-system --timeout=600s

              echo "Cleaning up bootstrap resources..."
              kubectl delete job cilium-install -n kube-system || true
              kubectl delete sa cilium-install -n kube-system || true
              kubectl delete clusterrolebinding cilium-install || true

              echo "Cleanup completed successfully."
---
# Finally, creates a loadbalancer IP pool so that Cilium can assign external IPs to LoadBalancer services
apiVersion: cilium.io/v2
kind: CiliumLoadBalancerIPPool
metadata:
  name: default
spec:
  blocks:
    - cidr: "192.168.1.10/32"